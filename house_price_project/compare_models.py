from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
import pandas as pd
from sklearn.model_selection import cross_val_score
import numpy as np

# åŠ è½½æ•°æ®
df = pd.read_csv('house_data.csv')
X = df.drop('price', axis=1)
y = df['price']

# å®šä¹‰è¦æ¯”è¾ƒçš„æ¨¡å‹
models = {
    'çº¿æ€§å›å½’': LinearRegression(),
    'å†³ç­–æ ‘': DecisionTreeRegressor(max_depth=5, random_state=42),
    'éšæœºæ£®æ—': RandomForestRegressor(n_estimators=100, random_state=42),
    'æ¢¯åº¦æå‡': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)
}

print("ğŸ”¬ æ¨¡å‹æ¯”è¾ƒï¼ˆ5æŠ˜äº¤å‰éªŒè¯ï¼‰")
print("=" * 50)

results = []
for name, model in models.items():
    # ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°
    scores = cross_val_score(model, X, y, cv=5, scoring='r2')
    mae_scores = -cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')
    
    results.append({
        'æ¨¡å‹': name,
        'å¹³å‡RÂ²': scores.mean(),
        'RÂ²æ ‡å‡†å·®': scores.std(),
        'å¹³å‡MAE(å…ƒ)': mae_scores.mean()
    })
    
    print(f"{name:15s}: RÂ² = {scores.mean():.4f} (Â±{scores.std():.4f}), "
          f"MAE = {mae_scores.mean():,.0f}å…ƒ")

# æ˜¾ç¤ºæœ€ä½³æ¨¡å‹
results_df = pd.DataFrame(results).sort_values('å¹³å‡RÂ²', ascending=False)
print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {results_df.iloc[0]['æ¨¡å‹']} (RÂ² = {results_df.iloc[0]['å¹³å‡RÂ²']:.4f})")

# è®­ç»ƒå¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
best_model_name = results_df.iloc[0]['æ¨¡å‹']
best_model = models[best_model_name]
best_model.fit(X, y)

import joblib
joblib.dump(best_model, f'best_model_{best_model_name}.pkl')
print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ä¸º: best_model_{best_model_name}.pkl")